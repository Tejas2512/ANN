{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary library.\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset.\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train_full,y_train_full),(x_test,y_test) = fashion_mnist.load_data()\n",
    "x_valid, x_train = x_train_full[:5000]/255.0, x_train_full[5000:]/255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "classes_name = ['top','trouser','pullover','dress','coat','sandel','shirt','sneaker','bag','ankel_foot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model buliding\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(300, kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('elu'))\n",
    "model.add(keras.layers.Dense(100, kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.Activation('elu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why batch normalization?\n",
    "> prevent from Internal Covariate Shift\n",
    "\n",
    "The key issue that batch normalisation tackles is internal covariate shift. Internal covariate shift occurs due to the very nature of neural networks. At every epoch of training, weights are updated and different data is being processed, which means that the inputs to a neuron is slightly different every time. As these changes get passed on to the next neuron, it creates a situation where the input distribution of every neuron is different at every epoch.\n",
    "\n",
    "Normally, this is not a big deal, but in deep networks, these small changes in input distribution add up fast and amplify greatly deeper into the network. Ultimately, the input distribution received by the deepest neurons changes greatly between every epoch.\n",
    "\n",
    "As a result, these neurons need to continuously adapt to the changing input distribution, meaning that their learning capabilities are severely bottlenecked. This constantly changing input distribution is called internal covariate shift.\n",
    "\n",
    "> To avoid vanishing and exploding gradient problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient clipping\n",
    "> Another popular technique ti lessen exploding gradient problem is to simple clip the gradient during backpropagation so that they never exceed some thresold this is called **gradient clipping**. This method impelemented as follow:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(clipvalue=1.0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> this will clip every component of gradient between -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_8 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If your targets are one-hot encoded, use categorical_crossentropy.\n",
    "\n",
    "> But if your targets are integers, use sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is function for learning rate scheduler \n",
    "# learning rate exponatialy decrease after each epoch \n",
    "def decay(lr0,s):\n",
    "    def exponential_decay(epoch):\n",
    "        return lr0*0.1*(epoch/s)\n",
    "    return exponential_decay\n",
    "\n",
    "exponential_decay_fn = decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/200\n",
      "55000/55000 [==============================] - 16s 292us/step - loss: 2.9529 - accuracy: 0.0885 - val_loss: 2.9298 - val_accuracy: 0.0862\n",
      "Epoch 2/200\n",
      "55000/55000 [==============================] - 15s 272us/step - loss: 0.6964 - accuracy: 0.7649 - val_loss: 0.4839 - val_accuracy: 0.8378\n",
      "Epoch 3/200\n",
      "55000/55000 [==============================] - 15s 265us/step - loss: 0.4707 - accuracy: 0.8357 - val_loss: 0.3947 - val_accuracy: 0.8656\n",
      "Epoch 4/200\n",
      "55000/55000 [==============================] - 15s 264us/step - loss: 0.4094 - accuracy: 0.8558 - val_loss: 0.3726 - val_accuracy: 0.8674\n",
      "Epoch 5/200\n",
      "55000/55000 [==============================] - 14s 262us/step - loss: 0.3765 - accuracy: 0.8648 - val_loss: 0.3416 - val_accuracy: 0.8784\n",
      "Epoch 6/200\n",
      "55000/55000 [==============================] - 15s 265us/step - loss: 0.3501 - accuracy: 0.8737 - val_loss: 0.3524 - val_accuracy: 0.8726\n",
      "Epoch 7/200\n",
      "55000/55000 [==============================] - 14s 263us/step - loss: 0.3337 - accuracy: 0.8786 - val_loss: 0.3211 - val_accuracy: 0.8856\n",
      "Epoch 8/200\n",
      "55000/55000 [==============================] - 14s 261us/step - loss: 0.3169 - accuracy: 0.8847 - val_loss: 0.3177 - val_accuracy: 0.8866\n",
      "Epoch 9/200\n",
      "55000/55000 [==============================] - 14s 261us/step - loss: 0.3065 - accuracy: 0.8879 - val_loss: 0.3154 - val_accuracy: 0.8856\n",
      "Epoch 10/200\n",
      "55000/55000 [==============================] - 14s 261us/step - loss: 0.2911 - accuracy: 0.8930 - val_loss: 0.3217 - val_accuracy: 0.8830\n",
      "Epoch 11/200\n",
      "55000/55000 [==============================] - 14s 263us/step - loss: 0.2859 - accuracy: 0.8940 - val_loss: 0.3308 - val_accuracy: 0.8810\n",
      "Epoch 12/200\n",
      "55000/55000 [==============================] - 14s 258us/step - loss: 0.2739 - accuracy: 0.8975 - val_loss: 0.3081 - val_accuracy: 0.8910\n",
      "Epoch 13/200\n",
      "55000/55000 [==============================] - 14s 263us/step - loss: 0.2666 - accuracy: 0.9019 - val_loss: 0.3171 - val_accuracy: 0.8880\n",
      "Epoch 14/200\n",
      "55000/55000 [==============================] - 14s 260us/step - loss: 0.2599 - accuracy: 0.9033 - val_loss: 0.3005 - val_accuracy: 0.8904\n",
      "Epoch 15/200\n",
      "55000/55000 [==============================] - 14s 261us/step - loss: 0.2533 - accuracy: 0.9055 - val_loss: 0.3081 - val_accuracy: 0.8886\n",
      "Epoch 16/200\n",
      "55000/55000 [==============================] - 15s 266us/step - loss: 0.2464 - accuracy: 0.9080 - val_loss: 0.2989 - val_accuracy: 0.8952\n",
      "Epoch 17/200\n",
      "55000/55000 [==============================] - 15s 269us/step - loss: 0.2425 - accuracy: 0.9112 - val_loss: 0.3036 - val_accuracy: 0.8896\n",
      "Epoch 18/200\n",
      "55000/55000 [==============================] - 15s 266us/step - loss: 0.2349 - accuracy: 0.9125 - val_loss: 0.2973 - val_accuracy: 0.8932\n",
      "Epoch 19/200\n",
      "55000/55000 [==============================] - 15s 265us/step - loss: 0.2303 - accuracy: 0.9123 - val_loss: 0.2951 - val_accuracy: 0.8954\n",
      "Epoch 20/200\n",
      "55000/55000 [==============================] - 14s 263us/step - loss: 0.2252 - accuracy: 0.9147 - val_loss: 0.3163 - val_accuracy: 0.8898\n",
      "Epoch 21/200\n",
      "55000/55000 [==============================] - 15s 272us/step - loss: 0.2206 - accuracy: 0.9173 - val_loss: 0.3156 - val_accuracy: 0.8890\n",
      "Epoch 22/200\n",
      "55000/55000 [==============================] - 15s 271us/step - loss: 0.2167 - accuracy: 0.9179 - val_loss: 0.3018 - val_accuracy: 0.8954\n",
      "Epoch 23/200\n",
      "55000/55000 [==============================] - 15s 266us/step - loss: 0.2154 - accuracy: 0.9195 - val_loss: 0.3063 - val_accuracy: 0.8910\n",
      "Epoch 24/200\n",
      "55000/55000 [==============================] - 14s 263us/step - loss: 0.2064 - accuracy: 0.9229 - val_loss: 0.3258 - val_accuracy: 0.8840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x208027dc550>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=200,\n",
    "          validation_data=(x_valid,y_valid),\n",
    "          callbacks=[keras.callbacks.EarlyStopping(patience=5),keras.callbacks.LearningRateScheduler(exponential_decay_fn)]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### performance scheduling\n",
    "> performance scheduling use the ReduceLROnPlateau callback. if you pass following callback to fit method, it will multiply learning rate by 0.5 whenever the best validation loss does not improve for 5 consecutive epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.ReduceLROnPlateau at 0x20802280a90>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### picewise scheduling\n",
    "> For picewise scheduling, you can use following one then create LearningRateScheduler callback with this function and pass it to fit method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def picewise_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponential scheduling\n",
    "> For Exponential scheduling, you can use following one then create LearningRateScheduler callback with this function and pass it to fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay(lr0,s):\n",
    "    def exponential_decay(epoch):\n",
    "        return lr0*0.1*(epoch/s)\n",
    "    return exponential_decay\n",
    "\n",
    "exponential_decay_fn = decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD at 0x2082647f8e0>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 10\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.01,s,0.1)\n",
    "tf.keras.optimizers.SGD(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
